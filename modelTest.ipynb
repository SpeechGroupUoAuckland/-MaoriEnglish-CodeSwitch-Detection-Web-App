{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer\n",
    "from transformers import AutoModelForSequenceClassification\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import pickle\n",
    "import tensorflow as tf\n",
    "from keras.utils.data_utils import pad_sequences\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read test set\n",
    "test = pd.read_csv('20220321_Hansard_DB_test_MP_only.csv')\n",
    "# only take the rows with Processed = 'Y'\n",
    "test = test[test['Processed'] == 'Y']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_folder = './web/models/'\n",
    "# Parameters #\n",
    "full_size_bilstm_model_path = base_folder+'bilstm.h5'\n",
    "full_size_bilstm_tokenizer_path = base_folder+'tokenizerBilstm.pickle'\n",
    "full_size_bilstm_lower_model_path = base_folder+'bilstmLower.h5'\n",
    "full_size_bilstm_lower_tokenizer_path = base_folder+'tokenizerBilstmLower.pickle'\n",
    "\n",
    "size_2_bilstm_model_path = base_folder+'bilstmSize2.h5'\n",
    "size_2_bilstm_tokenizer_path = base_folder+'tokenizerBilstmSize2.pickle'\n",
    "size_2_bilstm_lower_model_path = base_folder+'bilstmSize2Lower.h5'\n",
    "size_2_bilstm_lower_tokenizer_path = base_folder+'tokenizerBilstmSize2Lower.pickle'\n",
    "\n",
    "size_3_bilstm_model_path = base_folder+'bilstmSize3.h5'\n",
    "size_3_bilstm_tokenizer_path = base_folder+'tokenizerBilstmSize3.pickle'\n",
    "size_3_bilstm_lower_model_path = base_folder+'bilstmSize3Lower.h5'\n",
    "size_3_bilstm_lower_tokenizer_path = base_folder+'tokenizerBilstmSize3Lower.pickle'\n",
    "\n",
    "full_size_mbert_model_path = base_folder+'mbert'\n",
    "full_size_mbert_lower_model_path = base_folder+'mbertLower'\n",
    "\n",
    "mbert_tokenizer_path = base_folder+'tokenizerMbert'\n",
    "# End of parameters #\n",
    "\n",
    "# Load models #\n",
    "with tf.device('/cpu:0'):\n",
    "    full_size_bilstm_model = tf.keras.models.load_model(full_size_bilstm_model_path)\n",
    "    full_size_bilstm_lower_model = tf.keras.models.load_model(full_size_bilstm_lower_model_path)\n",
    "\n",
    "    size_2_bilstm_model = tf.keras.models.load_model(size_2_bilstm_model_path)\n",
    "    size_2_bilstm_lower_model = tf.keras.models.load_model(size_2_bilstm_lower_model_path)\n",
    "\n",
    "    size_3_bilstm_model = tf.keras.models.load_model(size_3_bilstm_model_path)\n",
    "    size_3_bilstm_lower_model = tf.keras.models.load_model(size_3_bilstm_lower_model_path)\n",
    "\n",
    "with open(full_size_bilstm_tokenizer_path, 'rb') as handle:\n",
    "    full_size_bilstm_tokenizer = pickle.load(handle)\n",
    "with open(full_size_bilstm_lower_tokenizer_path, 'rb') as handle:\n",
    "    full_size_bilstm_lower_tokenizer = pickle.load(handle)\n",
    "\n",
    "with open(size_2_bilstm_tokenizer_path, 'rb') as handle:\n",
    "    size_2_bilstm_tokenizer = pickle.load(handle)\n",
    "with open(size_2_bilstm_lower_tokenizer_path, 'rb') as handle:\n",
    "    size_2_bilstm_lower_tokenizer = pickle.load(handle)\n",
    "\n",
    "with open(size_3_bilstm_tokenizer_path, 'rb') as handle:\n",
    "    size_3_bilstm_tokenizer = pickle.load(handle)\n",
    "with open(size_3_bilstm_lower_tokenizer_path, 'rb') as handle:\n",
    "    size_3_bilstm_lower_tokenizer = pickle.load(handle)\n",
    "\n",
    "full_size_mbert_model = AutoModelForSequenceClassification.from_pretrained(full_size_mbert_model_path, num_labels=3)\n",
    "for param in full_size_mbert_model.parameters():\n",
    "    param.requires_grad_(False)\n",
    "full_size_mbert_lower_model = AutoModelForSequenceClassification.from_pretrained(full_size_mbert_lower_model_path, num_labels=3)\n",
    "for param in full_size_mbert_lower_model.parameters():\n",
    "    param.requires_grad_(False)\n",
    "\n",
    "mbert_tokenizer = AutoTokenizer.from_pretrained(mbert_tokenizer_path)\n",
    "# End of load models #\n",
    "\n",
    "# Functions #\n",
    "## Clean the text\n",
    "def cleanText(text):\n",
    "    text = text.replace(\"“\", '\"').replace(\n",
    "        \"”\", '\"').replace(\"‘\", \"'\").replace(\"’\", \"'\")\n",
    "    # replace non ascii char but keep the maori chars\n",
    "    text = re.sub(r'[^\\x00-\\x7FāēīōūĀĒĪŌŪ]+', '', text)\n",
    "    text = text.replace('\\r', '  ').replace(\n",
    "        '\\n', '  ').replace('\\t', '  ')  # remove \\r \\n \\t\n",
    "    text = text.replace(':', ': ').replace(';', '; ').replace(\n",
    "        ',', ', ').replace('.', '. ')  # add space after the symbols\n",
    "    while '  ' in text:\n",
    "        text = text.replace('  ',  ' ')  # remove redundant spaces\n",
    "    text = text.replace(' :', ':').replace(' ;', ';').replace(\n",
    "        ' ,', ',').replace(' .', '.')  # remove space before the symbols\n",
    "    # handle a.m and p.m\n",
    "    text = text.replace('a. m', 'a.m').replace('p. m', 'p.m')\n",
    "    return text.strip()\n",
    "\n",
    "## BiLSTM model ##\n",
    "### Detect the code switching point in a dynamic window\n",
    "def sentenceCategory(sentence, padding_length, tokenizer, loaded_model):\n",
    "    seq = tokenizer.texts_to_sequences([sentence])\n",
    "    padded = pad_sequences(seq, maxlen=padding_length)\n",
    "    predict = loaded_model.predict(padded, verbose = 0) \n",
    "    classw = np.argmax(predict,axis=1)\n",
    "    return int(classw[0])\n",
    "\n",
    "def detectCodeSwitchingPointDynamicWindowVersion(x, w, tokenizer, loaded_model):\n",
    "    p = w\n",
    "    words_list = x.split()\n",
    "    end = len(words_list)\n",
    "    if w >= end and end > 2:\n",
    "        w = end - 1\n",
    "    elif end == 1:\n",
    "        w = 1\n",
    "    elif end == 2:\n",
    "        w = 2\n",
    "    else:\n",
    "        pass\n",
    "\n",
    "    if end < 1:\n",
    "        return []\n",
    "\n",
    "    elif end < 2:\n",
    "        if re.search(u'[āēīōūĀĒĪŌŪ]', x):\n",
    "            return [1]\n",
    "        elif re.search(u'[bBcCdDfFgGjJlLqQsSvVxXyYzZ]', x):\n",
    "            return [2]\n",
    "        else:\n",
    "            return [sentenceCategory(x, p, tokenizer, loaded_model)]\n",
    "\n",
    "    elif end == 2:\n",
    "        if not re.search(u'[āēīōūĀĒĪŌŪ]', x):\n",
    "            tmp_result = sentenceCategory(x, p, tokenizer, loaded_model)\n",
    "            if tmp_result == 1 and not re.search(u'[bBcCdDfFgGjJlLqQsSvVxXyYzZ]', x):\n",
    "                return [1, 1]\n",
    "            elif tmp_result == 2:\n",
    "                return [2, 2]\n",
    "            else:\n",
    "                if sentenceCategory(words_list[0], p, tokenizer, loaded_model) == 1 and not re.search(u'[bBcCdDfFgGjJlLqQsSvVxXyYzZ]', words_list[0]):\n",
    "                    return [1, 2]\n",
    "                else:\n",
    "                    return [2, 1]\n",
    "        else:\n",
    "            tmp_char_0 = re.search(u'[āēīōūĀĒĪŌŪ]', words_list[0])\n",
    "            tmp_char_1 = re.search(u'[āēīōūĀĒĪŌŪ]', words_list[1])\n",
    "            if tmp_char_0 and tmp_char_1:\n",
    "                return [1, 1]\n",
    "            if tmp_char_0 and not tmp_char_1:\n",
    "                return [1, 2]\n",
    "            else:\n",
    "                return [2, 1]\n",
    "    \n",
    "    else:\n",
    "        result = []\n",
    "        ptr = 0\n",
    "        while ptr < end:\n",
    "            this_window = words_list[ptr:ptr+w]\n",
    "            if ptr + w > end:\n",
    "                w = end - ptr\n",
    "            else:\n",
    "                pass\n",
    "\n",
    "            tmp_result = sentenceCategory(\" \".join(this_window), p, tokenizer, loaded_model)\n",
    "            if tmp_result == 1 and not re.search(u'[bBcCdDfFgGjJlLqQsSvVxXyYzZ]', \" \".join(this_window)):\n",
    "                result.extend([1 for _ in range(w)])\n",
    "            elif tmp_result == 2 and not re.search(u'[āēīōūĀĒĪŌŪ]', \" \".join(this_window)):\n",
    "                result += [2 for _ in range(w)]\n",
    "            else:\n",
    "                if w >= 4 and w % 2 == 0:\n",
    "                    result += detectCodeSwitchingPointDynamicWindowVersion(\" \".join(this_window), w-2, tokenizer, loaded_model)\n",
    "                elif w > 1:\n",
    "                    result += detectCodeSwitchingPointDynamicWindowVersion(\" \".join(this_window), w-1, tokenizer, loaded_model)\n",
    "                else:\n",
    "                    result += detectCodeSwitchingPointDynamicWindowVersion(\" \".join(this_window), w, tokenizer, loaded_model)\n",
    "            ptr += w\n",
    "        return result\n",
    "## End of BiLSTM model ##\n",
    "\n",
    "## MBERT model ##\n",
    "@torch.no_grad()\n",
    "def sentenceCategoryMbertVersion(text: str, model) -> int:\n",
    "    tokenized_text = mbert_tokenizer(text, padding=\"longest\", truncation=True, return_tensors='pt')\n",
    "    prediction = model(input_ids=tokenized_text[\"input_ids\"], attention_mask=tokenized_text[\"attention_mask\"], token_type_ids=tokenized_text[\"token_type_ids\"])\n",
    "    return prediction.logits.detach().cpu().numpy().argmax()\n",
    "\n",
    "def detectCodeSwitchingPointMbertVersion(x: str, w: int, model) -> list():\n",
    "    words_list = x.split()\n",
    "    end = len(words_list)\n",
    "    if w >= end and end > 2:\n",
    "        w = end - 1\n",
    "    elif end == 1:\n",
    "        w = 1\n",
    "    elif end == 2:\n",
    "        w = 2\n",
    "    else:\n",
    "        pass\n",
    "\n",
    "    if end < 1:\n",
    "        return []\n",
    "\n",
    "    elif end == 1:\n",
    "        if re.search(u'[āēīōūĀĒĪŌŪ]', x):\n",
    "            return [1]\n",
    "        elif re.search(u'[bBcCdDfFgGjJlLqQsSvVxXyYzZ]', x):\n",
    "            return [2]\n",
    "        else:\n",
    "            return [sentenceCategoryMbertVersion(x, model)]\n",
    "\n",
    "    elif end == 2:\n",
    "        if not re.search(u'[āēīōūĀĒĪŌŪ]', x):\n",
    "            tmp_result = sentenceCategoryMbertVersion(x, model)\n",
    "            if tmp_result == 1 and not re.search(u'[bBcCdDfFgGjJlLqQsSvVxXyYzZ]', x):\n",
    "                return [1, 1]\n",
    "            elif tmp_result == 2:\n",
    "                return [2, 2]\n",
    "            else:\n",
    "                if sentenceCategoryMbertVersion(words_list[0], model) == 1 and not re.search(u'[bBcCdDfFgGjJlLqQsSvVxXyYzZ]', words_list[0]):\n",
    "                    return [1, 2]\n",
    "                else:\n",
    "                    return [2, 1]\n",
    "        else:\n",
    "            if re.search(u'[āēīōūĀĒĪŌŪ]', words_list[0]) and re.search(u'[āēīōūĀĒĪŌŪ]', words_list[1]):\n",
    "                return [1, 1]\n",
    "            if re.search(u'[āēīōūĀĒĪŌŪ]', words_list[0]) and not re.search(u'[āēīōūĀĒĪŌŪ]', words_list[1]):\n",
    "                return [1, 2]\n",
    "            else:\n",
    "                return [2, 1]\n",
    "    \n",
    "    else:\n",
    "        result = []\n",
    "        ptr = 0\n",
    "        while ptr < end:\n",
    "            this_window = words_list[ptr:ptr+w]\n",
    "            if ptr + w > end:\n",
    "                w = end - ptr\n",
    "            else:\n",
    "                pass\n",
    "            if sentenceCategoryMbertVersion(\" \".join(this_window), model) == 1 and not re.search(u'[bBcCdDfFgGjJlLqQsSvVxXyYzZ]', \" \".join(this_window)):\n",
    "                result.extend([1 for _ in range(w)])\n",
    "            elif sentenceCategoryMbertVersion(\" \".join(this_window), model) == 2 and not re.search(u'[āēīōūĀĒĪŌŪ]', \" \".join(this_window)):\n",
    "                result += [2 for _ in range(w)]\n",
    "            else:\n",
    "                print(w)\n",
    "                if w >= 4 and w % 2 == 0:\n",
    "                    result += detectCodeSwitchingPointMbertVersion(\" \".join(this_window), w-2, model)\n",
    "                elif w > 1:\n",
    "                    result += detectCodeSwitchingPointMbertVersion(\" \".join(this_window), w-1, model)\n",
    "                else:\n",
    "                    result += detectCodeSwitchingPointMbertVersion(\" \".join(this_window), w, model)\n",
    "            ptr += w\n",
    "        return result\n",
    "## End of Mbert model ##\n",
    "\n",
    "def transfrom(a: list) -> list:\n",
    "    for index, item in enumerate(a):\n",
    "        if item == 1:\n",
    "            a[index] = 'M'\n",
    "        elif item == 2:\n",
    "            a[index] = 'P'\n",
    "        else:\n",
    "            a[index] = 'U'\n",
    "    return a\n",
    "# End of functions #"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_model(model_name, window_size):\n",
    "    model = globals()[f'{model_name}_model']\n",
    "    tokenizer = globals()[f'{model_name}_tokenizer']\n",
    "\n",
    "    lower_flag = True if 'lower' in model_name else False\n",
    "\n",
    "    word_count = 0\n",
    "    wrong_word_count = 0\n",
    "    wrong_word_dict = {} # {word1: count1, word2: count2, ...}\n",
    "\n",
    "    sentence_count = 0\n",
    "    wrong_sentence_count = 0\n",
    "    wrong_sentence_list = [] # [row.id1, row.id2, ...]\n",
    "\n",
    "    for row in test.itertuples():\n",
    "        text = row.text.lower() if lower_flag else row.text\n",
    "        predict = transfrom(detectCodeSwitchingPointDynamicWindowVersion(text, window_size, tokenizer, model))\n",
    "        real = list(row.label)\n",
    "        if len(predict) == len(real):\n",
    "            wrong_sentence = False\n",
    "            for index, item in enumerate(predict):\n",
    "                word_count += 1\n",
    "                if item != real[index]:\n",
    "                    wrong_sentence = True\n",
    "                    wrong_word_count += 1\n",
    "                    current_word = row.text.split()[index].lower()\n",
    "                    if current_word not in wrong_word_dict:\n",
    "                        wrong_word_dict[current_word] = 1\n",
    "                    else:\n",
    "                        wrong_word_dict[current_word] += 1\n",
    "            if wrong_sentence:\n",
    "                wrong_sentence_count += 1\n",
    "                wrong_sentence_list.append(row.id)\n",
    "            sentence_count += 1\n",
    "        else:\n",
    "            # print(\"Error: length of predict and real is not equal,\", row.id)\n",
    "            pass\n",
    "\n",
    "    # Save the wrong words and wrong sentences\n",
    "    with open(f\"evaluation/{model_name}_error_dict.json\", \"w\") as f:\n",
    "        f.write('{\\n'+f'\"{wrong_word_count}/{word_count}\":')\n",
    "        json.dump(wrong_word_dict, f)\n",
    "        f.write(f',\\n\"{wrong_sentence_count}/{sentence_count}\":')\n",
    "        json.dump(wrong_sentence_list, f)\n",
    "        f.write('\\n}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import multiprocessing\n",
    "\n",
    "pool = multiprocessing.Pool(processes=6)\n",
    "\n",
    "pool.starmap(test_model, [('full_size_bilstm', 250), ('full_size_bilstm_lower', 250), ('size_2_bilstm', 2), ('size_2_bilstm_lower', 2), ('size_3_bilstm', 3), ('size_3_bilstm_lower', 3)])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('dev')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "48caaebc5ae0c247be4f972c3642ff874087c1f9cc458b6f1157e23ba1abcd1f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
